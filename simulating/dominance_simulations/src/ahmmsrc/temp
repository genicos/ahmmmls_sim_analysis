#ifndef OPTIMIZE_SELECTION
#define OPTIMIZE_SELECTION
#include <vector>
#include <math.h>



#include <time.h>
#include <sys/time.h>
double get_wall_time(){
    struct timeval time;
    if (gettimeofday(&time,NULL)){
        //  Handle error
        return 0;
    }
    return (double)time.tv_sec + (double)time.tv_usec * .000001;
}


selection_opt context;

void selection_opt::set_context(){
    context = *this;
}


double timer = 0;



vector<double> get_local_ancestry(vector<mat> neutral_model){
    
    map<int,vector<mat> > transition_matrix ;

    alt_create_transition_matrix( transition_matrix, context.transition_matrix_information[context.markov_chain_information[0].ploidy_switch[0]], context.n_recombs, context.position, context.markov_chain_information[0].ploidy_switch[0], neutral_model ) ;

    vector<mat> interploidy_transitions;


    int sample_count = context.markov_chain_information.size();
    int site_count = context.markov_chain_information[0].alphas.size();


    //populating alphas
    for ( int m = 0 ; m < context.markov_chain_information.size() ; m ++ ) {
        context.markov_chain_information[m].compute_forward_probabilities(  transition_matrix, interploidy_transitions) ;
    }

    //populating betas
    for ( int m = 0 ; m < context.markov_chain_information.size() ; m ++ ) {
        context.markov_chain_information[m].compute_backward_probabilities( transition_matrix, interploidy_transitions ) ;
    }

    
    vector<double> expected_ancestry(context.markov_chain_information[0].alphas.size());

    //looping through samples
    for ( int m = 0 ; m < context.markov_chain_information.size() ; m ++ ) {

        //looping through sites
        for( int i = 0; i < expected_ancestry.size(); i++){
            vec smoothed_probs = context.markov_chain_information[m].alphas[i] % context.markov_chain_information[m].betas[i] ;
            normalize( smoothed_probs ) ;

            //ploidy 1
            if(smoothed_probs.size() == 2){
                expected_ancestry[i] += smoothed_probs[0];
            }

            //ploidy 2
            if(smoothed_probs.size() == 3){
                expected_ancestry[i] += smoothed_probs[0];
                expected_ancestry[i] += smoothed_probs[1] * 0.5;
            }

            //TODO generalize ploidy here
        }

    }

    for( int i = 0; i < expected_ancestry.size(); i++){
        expected_ancestry[i] /= sample_count;
    }

    return expected_ancestry;
}





vector<mat> last_calculated_transition_matricies;

double to_be_optimized(vector<double> parameters){

    int selected_sites_count = parameters.size()/3;
    vector<double> selection_recomb_rates(selected_sites_count + 1);
    vector<vector<double>> fitnesses(selected_sites_count);

    if(selected_sites_count > 0){
        
        cerr << "Calculating likelihood for\n";
        for (uint i = 0; i < selected_sites_count; i++){
            cerr << "selection site: " << parameters[3*i + 0] << " with fitness: " << parameters[3*i + 1] << ",1," << parameters[3*i + 2] << "\n";
        }
        //Sort selected sites and fitnesses///////////////////////////
        struct Selected_pair{                                       //
            double site;
            vector<double> fitness;

            bool operator<(const Selected_pair x) const
                { return site < x.site;}
        };
        
        //Fill vector of structs
        vector<Selected_pair> selected_pairs(selected_sites_count);
        for(int i = 0; i < selected_sites_count; i++){
            Selected_pair ss;
            ss.site = parameters[i*3];
            ss.fitness.resize(3);
            ss.fitness[0] = parameters[i*3 + 1];
            ss.fitness[1] = 1;
            ss.fitness[2] = parameters[i*3 + 2];

            selected_pairs[i] = ss;
        }
              
        //sort
        sort(selected_pairs.begin(), selected_pairs.end());         //
        //////////////////////////////////////////////////////////////


        double last = 0;
        for (uint i = 0; i < selected_pairs.size(); i++){
            selection_recomb_rates[i] = selected_pairs[i].site - last;
            last = selected_pairs[i].site;
        }

        selection_recomb_rates[selected_pairs.size()] = 1 - last;

        for(uint i = 0; i < selected_sites_count; i++){
            fitnesses[i] = selected_pairs[i].fitness;
        }
        
    }else{
        selection_recomb_rates[0] = 1;
    }


    vector<mat> transition_matrices = calculate_transition_rates(
        context.n_recombs,
        selection_recomb_rates,
        fitnesses,
        context.options.m,
        context.options.generations
    );
    last_calculated_transition_matricies = transition_matrices;
    
    //sometimes i need this? sometimes i dont?
    //No i definitely need this
    cerr << '\n';
    


    map<int,vector<mat> > transition_matrix ;

    alt_create_transition_matrix( transition_matrix, context.transition_matrix_information[context.markov_chain_information[0].ploidy_switch[0]], context.n_recombs, context.position, context.markov_chain_information[0].ploidy_switch[0], transition_matrices ) ;


    vector<mat> interploidy_transitions;

    
    double lnl = 0 ;
    
    
    for ( int m = 0 ; m < context.markov_chain_information.size() ; m ++ ) {
        lnl += context.markov_chain_information[m].compute_forward_probabilities( transition_matrix, interploidy_transitions) ;
    }


    cerr << "lnl = " << setprecision(15) << lnl << "\n";

    cerr << "TIME PASSED IN ONE ITERATION: " << (get_wall_time() - timer) << "\n";
    timer = get_wall_time();
    return lnl;
}


vector<mat> neutral_transition_matrices;

double to_be_optimized_only_near_sites(vector<double> parameters){

    int selected_sites_count = parameters.size()/3;
    vector<double> selection_recomb_rates(selected_sites_count + 1);
    vector<vector<double>> fitnesses(selected_sites_count);

    if(selected_sites_count > 0){

        cerr << "Calculating likelihood for\n";
        for (uint i = 0; i < selected_sites_count; i++){
            cerr << "selection site: " << parameters[3*i + 0] << " with fitness: " << parameters[i*3 + 1] << ",1," << parameters[i*3 + 2] << "\n";
        }

        //Sort selected sites and fitnesses///////////////////////////
        struct Selected_pair{                                       //
            double site;
            vector<double> fitness;

            bool operator<(const Selected_pair x) const
                { return site < x.site;}
        };
        
        //Fill vector of structs
        vector<Selected_pair> selected_pairs(selected_sites_count);
        for(int i = 0; i < selected_sites_count; i++){
            Selected_pair ss;
            ss.site = parameters[i*3];
            ss.fitness.resize(3);
            ss.fitness[0] = parameters[i*3 + 1];
            ss.fitness[1] = 1;
            ss.fitness[2] = parameters[3*i + 2];

            selected_pairs[i] = ss;
        }
              
        //sort
        sort(selected_pairs.begin(), selected_pairs.end());         //
        //////////////////////////////////////////////////////////////


        double last = 0;
        for (uint i = 0; i < selected_pairs.size(); i++){
            selection_recomb_rates[i] = selected_pairs[i].site - last;
            last = selected_pairs[i].site;
        }

        selection_recomb_rates[selected_pairs.size()] = 1 - last;

        for(uint i = 0; i < selected_sites_count; i++){
            fitnesses[i] = selected_pairs[i].fitness;
        }
        
    }else{
        selection_recomb_rates[0] = 1;
    }

    

    vector<mat> transition_matrices = fast_transition_rates(
        context.n_recombs,
        selection_recomb_rates,
        fitnesses,
        context.options.m,
        context.options.generations
    );

    
    //sometimes i need this? sometimes i dont?
    //No i definitely need this
    cerr << '\n';
    


    map<int,vector<mat> > transition_matrix ;

    alt_create_transition_matrix( transition_matrix, context.transition_matrix_information[context.markov_chain_information[0].ploidy_switch[0]], context.n_recombs, context.position, context.markov_chain_information[0].ploidy_switch[0], transition_matrices ) ;

    vector<mat> interploidy_transitions;

    double lnl = 0 ;
    for ( int m = 0 ; m < context.markov_chain_information.size() ; m ++ ) {
        lnl += context.markov_chain_information[m].compute_forward_probabilities( transition_matrix, interploidy_transitions) ;
    }



    
    vector<mat> these_neutral_transition_rates(neutral_transition_matrices.size());
    
    
    

    mat filler_matrix(2,2,fill::zeros);

    filler_matrix(0,0) = 0.5;
    filler_matrix(0,1) = 0.5;
    filler_matrix(1,0) = 0.5;
    filler_matrix(1,1) = 0.5;
    
    double sum = 0;
    for(uint i = 0; i < neutral_transition_matrices.size(); i++){
        sum += context.n_recombs[i];
        bool near_a_selected_site = false;
        for(uint j = 0; j < selected_sites_count; j++){
            double distance = sum - parameters[j*3];
            if (distance <= fast_transitions_radius_in_morgans && distance >= -fast_transitions_radius_in_morgans){
                near_a_selected_site = true;
            }
        }
        if(near_a_selected_site){
            these_neutral_transition_rates[i] = neutral_transition_matrices[i];
        }else{
            these_neutral_transition_rates[i] = filler_matrix;
        }
    }
    

    transition_matrix.clear();

    alt_create_transition_matrix( transition_matrix, context.transition_matrix_information[context.markov_chain_information[0].ploidy_switch[0]], context.n_recombs, context.position, context.markov_chain_information[0].ploidy_switch[0], these_neutral_transition_rates ) ;
    
    interploidy_transitions.clear();
    
    double neutral_lnl = 0 ;
    for ( int m = 0 ; m < context.markov_chain_information.size() ; m ++ ) {
        neutral_lnl += context.markov_chain_information[m].compute_forward_probabilities( transition_matrix, interploidy_transitions) ;
    }
    


    cerr << "lnl ratio = " << setprecision(15) << lnl - neutral_lnl << "\n";

    cerr << "TIME PASSED IN ONE ITERATION: " << (get_wall_time() - timer) << "\n";
    timer = get_wall_time();
    return lnl - neutral_lnl;
}


//This function updates local_ancestries as a side effect
double to_be_optimized_ignore_far_sites(vector<double> parameters){

    int selected_sites_count = parameters.size() / 3;
    vector<double> selection_recomb_rates(selected_sites_count + 1);
    vector<vector<double>> fitnesses(selected_sites_count);

    if(selected_sites_count > 0){

        cerr << "Calculating likelihood for\n";
        for (uint i = 0; i < selected_sites_count; i++){
            cerr << "selection site: " << parameters[3*i + 0] << " with fitness: " << parameters[i*3 + 1] << ",1," << parameters[i*3 + 2] << "\n";
        }

        //Sort selected sites and fitnesses///////////////////////////
        struct Selected_pair{                                       //
            double site;
            vector<double> fitness;

            bool operator<(const Selected_pair x) const
                { return site < x.site;}
        };
        
        //Fill vector of structs
        vector<Selected_pair> selected_pairs(selected_sites_count);
        for(int i = 0; i < selected_sites_count; i++){
            Selected_pair ss;
            ss.site = parameters[i*3];
            ss.fitness.resize(3);
            ss.fitness[0] = parameters[i*3 + 1];
            ss.fitness[1] = 1;
            ss.fitness[2] = parameters[3*i + 2];

            selected_pairs[i] = ss;
        }
              
        //sort
        sort(selected_pairs.begin(), selected_pairs.end());         //
        //////////////////////////////////////////////////////////////


        double last = 0;
        for (uint i = 0; i < selected_pairs.size(); i++){
            selection_recomb_rates[i] = selected_pairs[i].site - last;
            last = selected_pairs[i].site;
        }

        selection_recomb_rates[selected_pairs.size()] = 1 - last;

        for(uint i = 0; i < selected_sites_count; i++){
            fitnesses[i] = selected_pairs[i].fitness;
        }
        
    }else{
        selection_recomb_rates[0] = 1;
    }

    
    fast_transitions_radius_in_morgans = 0.1;
    vector<mat> transition_matrices = fast_transition_rates(
        context.n_recombs,
        selection_recomb_rates,
        fitnesses,
        context.options.m,
        context.options.generations
    );
    
    
    //sometimes i need this? sometimes i dont?
    //No i definitely need this
    cerr << '\n';
    

    double sum = 0;
    for(uint i = 0; i < neutral_transition_matrices.size(); i++){
        
        sum += context.n_recombs[i];

        bool near_a_selected_site = false;

        for(uint j = 0; j < selected_sites_count; j++){

            double distance = sum - parameters[j*3];

            if ( distance <=  fast_transitions_radius_in_morgans 
              && distance >= -fast_transitions_radius_in_morgans){

                near_a_selected_site = true;
            }
        }

        if(transition_matrices[i](0,0) == 0.5){
            
            transition_matrices[i] = neutral_transition_matrices[i];
            local_ancestries[i] = context.options.m;
        }
    }
    fast_transitions_radius_in_morgans = 0.01;


    last_calculated_transition_matricies = transition_matrices;



    map<int,vector<mat> > transition_matrix ;

    alt_create_transition_matrix( transition_matrix, context.transition_matrix_information[context.markov_chain_information[0].ploidy_switch[0]], context.n_recombs, context.position, context.markov_chain_information[0].ploidy_switch[0], transition_matrices ) ;


    vector<mat> interploidy_transitions;

    double lnl = 0 ;
    for ( int m = 0 ; m < context.markov_chain_information.size() ; m ++ ) {
        lnl += context.markov_chain_information[m].compute_forward_probabilities( transition_matrix, interploidy_transitions) ;
    }

    
    return lnl;
}







vector<double> sites_to_parameters(vector<vector<double>> sites){
    vector<double> parameters(sites.size()*3);

    for(uint i = 0; i < parameters.size(); i++){
        parameters[i] = sites[i/3][i%3];
    }

    return parameters;
}

vector<vector<double>> parameters_to_sites(vector<double> parameters){
    vector<vector<double>> sites(parameters.size()/3);

    for(int j = 0; j < sites.size(); j++){
        vector<double> site(3);
        site[0] = parameters[j*3 + 0];
        site[1] = parameters[j*3 + 1];
        site[2] = parameters[j*3 + 2];
        sites[j] = site;
    }

    return sites;
}





vector<double> search_site(double chrom_size, nelder_mead &opt, vector<double> site, double width, double height, double depth);
vector<double> search_sites(double chrom_size, nelder_mead &opt, vector<vector<double>> sites, double width, double height, double depth);
vector<double> search_sites_fast(double chrom_size, nelder_mead &opt, vector<vector<double>> sites, double width, double height, double depth);
vector<double> search_sites_fast_fix_all_but_last(double chrom_size, nelder_mead &opt, vector<vector<double>> sites, double width, double height, double depth);





vector<double> selection_opt::enact_optimization(){

    // Hyper Parameters of optimization ///////////
double nelder_mead_reflection  = 1;
double nelder_mead_contraction = 0.5;
double nelder_mead_expansion   = 2;
double nelder_mead_shrinkage   = 0.5;

int number_of_shallow_searches = 5;
double shallow_search_width = 0.03;
double shallow_search_height = 0.01;
double shallow_search_depth = 20;

double shallow_search_2_height = 0.04;

int number_of_deep_searches = 6;
double deep_search_width = 0.01;
double deep_search_height = 0.01;
double deep_search_depth = 5;
    //////////////////////////////////////////////



    // Create optimizer
    nelder_mead optimizer(
        nelder_mead_reflection,
        nelder_mead_contraction,
        nelder_mead_expansion,
        nelder_mead_shrinkage
    );

    //set_context();
    context = *this;
    
    
    double chrom_size = 0;
    for(uint i = 0; i < n_recombs.size(); i++){
        chrom_size += n_recombs[i];
    }

    //TESTER CODE
    if(true){
        vector<double> poss(3);
        poss[0] = 0.2;
        poss[1] = 1.01463655857229;
        poss[2] = 0.988099696707572;
        double poss_lnl = to_be_optimized(poss);

        vector<double> real(3);
        real[0] = 0.2;
        real[1] = 1.0204081;
        real[2] = 1;
        double real_lnl = to_be_optimized(real);
        cerr << "\n";

        cerr << "Poss point lnl: \t" << poss_lnl << "\n";
        cerr << "Real point lnl: \t" << real_lnl << "\n";
    }

    // Calculating lnl for neutral model
    vector<double> empty(0);                    
    double neutral_lnl = to_be_optimized(empty);
    

    cerr << "Neutral likelihood: " << setprecision(15) << neutral_lnl << "\n";

    neutral_transition_matrices = last_calculated_transition_matricies;




    






    //iterative selected site adding
    vector<vector<double>> sites;

    double last_lnl = neutral_lnl;
    cout << "Neutral lnl\t" << setprecision(15) << neutral_lnl << "\n";
    vector<double> data_ancestry;
    vector<double> expected_ancestry;

    //STANDARD: for now, only adds 5 sites
    for(int i = 0; i < 5; i++){

        data_ancestry = get_local_ancestry(neutral_transition_matrices);
        expected_ancestry = local_ancestries;

        double largest_deviation = 0;
        int largest_deviator = 0;
        for(int i = 0; i < n_recombs.size(); i++){
            if (abs(data_ancestry[i] - expected_ancestry[i]) > largest_deviation){
                largest_deviation = abs(data_ancestry[i] - expected_ancestry[i]);
                largest_deviator = i;
            }
            //if(i%100 == 0)
                //cerr << expected_ancestry[i] << "\n";
        }

        vector<double> new_site(3);
        new_site[0] = morgan_position[largest_deviator];
        new_site[1] = 1;
        new_site[2] = 1;
        sites.push_back(new_site);

        cerr << "placing at :\n";
        cerr << new_site[0] << "\n";
        cout << "Placing new site at: " << new_site[0] << "\n";

        vector<double> best_parameters;
        double best_ratio = -DBL_MAX;

        vector<double> found_parameters;



        //shallow search 1, around site guess with neutral center
        for(int k = 0; k < number_of_shallow_searches; k++){
            cerr << "\n SHORT SHALLOW SEARCH " << k + 1 << "/"<< number_of_shallow_searches << "\n\n";
            found_parameters = search_sites_fast_fix_all_but_last(chrom_size, optimizer, sites, shallow_search_width, shallow_search_height, shallow_search_depth);

            cout << "SHORT SHALLOW SEARCH " << k + 1 << "\n";
            cout << "lnl: " << optimizer.max_value << "\n";
            for(uint j = 0; j < found_parameters.size(); j++){
                cout << found_parameters[j] << "\n";
            }
            cout << "\n";

            
            if(optimizer.max_value > best_ratio) {
                best_ratio = optimizer.max_value;
                best_parameters = found_parameters;
            }
        }

        //shallow search 2, around site guess with neutral center
        for(int k = 0; k < number_of_shallow_searches; k++){
            cerr << "\n TALL SHALLOW SEARCH " << k + 1 << "/"<< number_of_shallow_searches << "\n\n";
            found_parameters = search_sites_fast_fix_all_but_last(chrom_size, optimizer, sites, shallow_search_width, shallow_search_2_height, shallow_search_depth);

            cout << "TALL SHALLOW SEARCH " << k + 1 << "\n";
            cout << "lnl: " << optimizer.max_value << "\n";
            for(uint j = 0; j < found_parameters.size(); j++){
                cout << found_parameters[j] << "\n";
            }
            cout << "\n";

            if(optimizer.max_value > best_ratio){
                best_ratio = optimizer.max_value;
                best_parameters = found_parameters;
            }
        }

            
        sites = parameters_to_sites(best_parameters);
        cerr << "\n SHALLOW SEARCH TERMINATED \n";
        cerr << "Best site found:\n";
        cerr << "site:\t" << sites[sites.size() - 1][0] << "\t" << sites[sites.size() - 1][1] << ",1," << sites[sites.size() - 1][2] << " or " << sites[sites.size() - 1][1]/max(sites[sites.size() - 1][1], sites[sites.size() - 1][2]) << ","<< 1/max(sites[sites.size() - 1][1], sites[sites.size() - 1][2]) <<"," << sites[sites.size() - 1][2]/max(sites[sites.size() - 1][1], sites[sites.size() - 1][2]) << "\n";




        //deep search, around best parameter of shallow search
        best_ratio = -DBL_MAX;
        for(int k = 0; k < number_of_deep_searches; k++){
            cerr << "\n DEEP SEARCH " << k + 1 << "/" << number_of_deep_searches << "\n\n";
            found_parameters = search_sites_fast_fix_all_but_last(chrom_size, optimizer, sites, deep_search_width, deep_search_height, deep_search_depth);
            
            cout << "DEEP SEARCH " << k + 1 << "\n";
            cout << "lnl: " << optimizer.max_value << "\n";
            for(uint j = 0; j < found_parameters.size(); j++){
                cout << found_parameters[j] << "\n";
            }
            cout << "\n";

            if(optimizer.max_value > best_ratio){
                best_ratio = optimizer.max_value;
                best_parameters = found_parameters;
            }
        }
        sites = parameters_to_sites(best_parameters);
        cerr << "\n DEEP SEARCH TERMINATED \n";
        cerr << "Best site found:\n";
        cerr << "site:\t" << sites[sites.size() - 1][0] << "\t" << sites[sites.size() - 1][1] << ",1," << sites[sites.size() - 1][2] << " or " << sites[sites.size() - 1][1]/max(sites[sites.size() - 1][1], sites[sites.size() - 1][2]) << ","<< 1/max(sites[sites.size() - 1][1], sites[sites.size() - 1][2]) <<"," << sites[sites.size() - 1][2]/max(sites[sites.size() - 1][1], sites[sites.size() - 1][2]) << "\n";



        cerr << "\n\nBest sites so far:\n";
        for(int k = 0; k < sites.size(); k++){

            cerr << "site:\t" << sites[k][0] << "\t" << sites[k][1] << ",1," << sites[k][2] << " or " << sites[k][1]/max(sites[k][1], sites[k][2]) << ","<< 1/max(sites[k][1], sites[k][2]) <<"," << sites[k][2]/max(sites[k][1], sites[k][2]) << "\n";
        }
        cerr << "\n";

        cout << "\nnew site added:\t" << sites[sites.size() - 1][0]
        << "\n" << sites[sites.size() - 1][1]
        << ",1," << sites[sites.size() - 1][2]
        << " or\n" << sites[sites.size() - 1][1]/max(sites[sites.size() - 1][1], sites[sites.size() - 1][2])
        << ","<< 1/max(sites[sites.size() - 1][1], sites[sites.size() - 1][2])
        << "," << sites[sites.size() - 1][2]/max(sites[sites.size() - 1][1], sites[sites.size() - 1][2]) << "\n";

        
        if(sites.size() > 1){
            //fine tuning, all sites are unfixed
            for(int k = 0; k < 3; k++){
                cerr << "\n FINE TUNING " << k + 1 << "/" << 3 << "\n\n";
                found_parameters = search_sites_fast(chrom_size, optimizer, sites, deep_search_width, deep_search_height, deep_search_depth);
                
                cout << "FINE TUNING " << k + 1 << "\n";
                cout << "lnl: " << optimizer.max_value << "\n";
                for(uint j = 0; j < found_parameters.size(); j++){
                    cout << found_parameters[j] << "\n";
                }
                cout << "\n";
                
                if(optimizer.max_value > best_ratio){
                    best_ratio = optimizer.max_value;
                    best_parameters = found_parameters;
                }
            }
            sites = parameters_to_sites(best_parameters);
            cerr << "\nFINE TUNING TERMINATED \n";
        }


        double new_lnl = to_be_optimized(best_parameters);

        cout <<"lnl after new site\t" << setprecision(15) << new_lnl << "\n";
        


        cerr << "lnl: " << new_lnl << "\n";

        if(new_lnl - last_lnl < 1){
            sites.pop_back();
            break;
        }
        last_lnl = new_lnl;

        expected_ancestry = local_ancestries;
    }

    cerr << "\n\nTERMINATED SEARCH\n\n\n";
    cout << "TERMINATED SEARCH\n";

    // I have to re-check sites by themselves to see if they really add to the lnl
        

    cerr << "Found sites:\n\n";
    for(int k = 0; k < sites.size(); k++){
        cout << "site:\t" << sites[k][0] << "\t" << sites[k][1] << ",1," << sites[k][2] << " or " << sites[k][1]/max(sites[k][1], sites[k][2]) << ","<< 1/max(sites[k][1], sites[k][2]) <<"," << sites[k][2]/max(sites[k][1], sites[k][2]) << "\n";
        cerr << "site:\t" << sites[k][0] << "\t" << sites[k][1] << ",1," << sites[k][2] << " or " << sites[k][1]/max(sites[k][1], sites[k][2]) << ","<< 1/max(sites[k][1], sites[k][2]) <<"," << sites[k][2]/max(sites[k][1], sites[k][2]) << "\n";
    }
    
    
    
    return sites_to_parameters(sites);
}





vector<double> search_sites(double chrom_size, nelder_mead &opt, vector<vector<double>> sites, double width, double height, double depth){

    vector<double> center_point(sites.size()*3);
    vector<double> scales(sites.size()*3);

    for(uint i = 0; i < sites.size(); i++){
        center_point[i*3+0] = sites[i][0];
        center_point[i*3+1] = sites[i][1];
        center_point[i*3+2] = sites[i][2];
        scales[i*3+0] = width;
        scales[i*3+1] = height;
        scales[i*3+2] = height;
    }

    opt.populate_points(sites.size()*3, 1, center_point, scales);
    
    opt.calculate_points(&to_be_optimized);

    while(opt.max_value - opt.min_value > depth){
        opt.iterate(&to_be_optimized);
        
        cerr << "INFO simplex_size: " << opt.simplex_size() << " output range: " << opt.max_value - opt.min_value << "\n";
    }

    return opt.points[opt.max_index];
}



vector<double> search_sites_fast(double chrom_size, nelder_mead &opt, vector<vector<double>> sites, double width, double height, double depth){

    vector<double> center_point(sites.size()*3);
    vector<double> scales(sites.size()*3);

    for(uint i = 0; i < sites.size(); i++){
        center_point[i*3+0] = sites[i][0];
        center_point[i*3+1] = sites[i][1];
        center_point[i*3+2] = sites[i][2];
        scales[i*3+0] = width;
        scales[i*3+1] = height;
        scales[i*3+2] = height;
    }

    opt.populate_points(sites.size()*3, 1, center_point, scales);

    //random reflections
    for(uint i = 0; i < center_point.size(); i++){
        if(rand() < RAND_MAX/2){
            for(uint j = 0; j < opt.points.size(); j++){
                opt.points[j][i] = 2*center_point[i] - opt.points[j][i];
            }
        }
    }

    opt.calculate_points(&to_be_optimized_only_near_sites);

    while(opt.max_value - opt.min_value > depth){
        opt.iterate(&to_be_optimized_only_near_sites);
        
        cerr << "INFO simplex_size: " << opt.simplex_size() << " output range: " << opt.max_value - opt.min_value << "\n";
    }

    return opt.points[opt.max_index];
}


vector<double> search_sites_fast_fix_all_but_last(double chrom_size, nelder_mead &opt, vector<vector<double>> sites, double width, double height, double depth){

    vector<vector<double>> new_sites;

    vector<bool> in_new_sites(sites.size());

    for(uint i = 0; i < sites.size(); i++){

        in_new_sites[i] = false;

        double distance_from_last = abs(sites[i][0] - sites[sites.size() - 1][0]);

        if (distance_from_last <= fast_transitions_radius_in_morgans) {
            new_sites.push_back(sites[i]);
            in_new_sites[i] = true;
        }

    }

    vector<double> center_point(new_sites.size()*3);
    vector<double> scales(new_sites.size()*3);

    for(uint i = 0; i < new_sites.size(); i++){
        center_point[i*3+0] = new_sites[i][0];
        center_point[i*3+1] = new_sites[i][1];
        center_point[i*3+2] = new_sites[i][2];
        scales[i*3+0] = width;
        scales[i*3+1] = height;
        scales[i*3+2] = height;
    }


    opt.populate_points(new_sites.size()*3, 1, center_point, scales);

    //random reflections
    for(uint i = 0; i < center_point.size(); i++){
        if(rand() < RAND_MAX/2){
            for(uint j = 0; j < opt.points.size(); j++){
                opt.points[j][i] = 2*center_point[i] - opt.points[j][i];
            }
        }
    }

    opt.calculate_points(&to_be_optimized_only_near_sites);

    while(opt.max_value - opt.min_value > depth){
        opt.iterate(&to_be_optimized_only_near_sites);
        cerr << "INFO simplex_size: " << opt.simplex_size() << " output range: " << opt.max_value - opt.min_value << "\n";
    }

    vector<double> ans(sites.size()*3);
    
    int counter = 0;

    for(uint i = 0; i < sites.size(); i++){
        if(in_new_sites[i]){
            ans[i*3 + 0] = opt.points[opt.max_index][counter*3 + 0];
            ans[i*3 + 1] = opt.points[opt.max_index][counter*3 + 1];
            ans[i*3 + 2] = opt.points[opt.max_index][counter*3 + 2];

            counter++;
        }else{ 
            ans[i*3 + 0] = sites[i][0];
            ans[i*3 + 1] = sites[i][1];
            ans[i*3 + 2] = sites[i][2];
        }
    }

    return ans;
}

#endif